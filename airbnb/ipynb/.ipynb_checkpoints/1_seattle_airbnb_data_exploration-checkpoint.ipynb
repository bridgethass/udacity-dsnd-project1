{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seattle AirBnB\n",
    "This notebook explores the Seattle Airbnb Open Data\n",
    "https://www.kaggle.com/airbnb/seattle/data\n",
    "\n",
    "First let's import some packages that will likely come in handy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-standard installations:\n",
    "`!conda install basemap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "calendar_df = pd.read_csv('../data/calendar.csv') \n",
    "listings_df = pd.read_csv('../data/listings.csv')\n",
    "reviews_df = pd.read_csv('../data/reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calendar_df.count())\n",
    "print(calendar_df.count()/calendar_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the price data (~33%) is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listings_df.columns)\n",
    "listings_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(listings_df.corr());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like some of these variables are highly correlated, mostly as we would expect -- for example, if one review score is high, the others are likely high as well (eg. rating, accuracy, cleanliness, checkin, communication, value); the location review is the least correlated to the others, which also makes sense. \n",
    "\n",
    "Likewise, availabilities are highly correlated (30, 60, 90, 365) as are the variables related to the house size (accomodates, bathrooms, bedrooms, beds, square feet). When determining predictor variables to use, we won't want to use all of the highly correlated values, but perhaps only one most representative value from each group. \n",
    "\n",
    "Let's look at a correlation matrix with a reduced number of variables, first let's find a list of the most highly correlated variables in a systematic way. Here I'm following steps from this \n",
    "https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = listings_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that we found the two most highly correlated features. Let's turn this into a function and test some different thresholds to see if we can get some of the other variables we saw that look like they have high correlations. Looking at the heatmap, let's try a correlation threshold of 0.8 to see what that gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_correlated_features(df,threshold):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    \n",
    "    return to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_cols = find_correlated_features(listings_df,0.8)\n",
    "print(corr_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this gave us a few more. Let's loop through a number of different thresholds from 0.5 to 0.9 and list the correlated columns for each threshold. We can then select the set that looks the best, upon comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.5,0.6,0.7,0.8,0.9]\n",
    "for t in thresholds:\n",
    "    cols = find_correlated_features(listings_df,t)\n",
    "    print(str(t) + ':' + ','.join(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop features with > 50% correlation\n",
    "cols_to_drop = find_correlated_features(listings_df,0.5)\n",
    "listings_df2 = listings_df.copy().drop(listings_df[cols_to_drop], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's also remove columns with missing values\n",
    "listings_df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df2 = listings_df2.drop(labels=['id','scrape_id','license'],axis=1)\n",
    "sns.heatmap(listings_df2.corr());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,15))\n",
    "listings_df.hist(ax = fig.gca());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_missing_cols = set(listings_df.columns[listings_df.isnull().mean() > 0.5])\n",
    "most_missing_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(listings_df.shape)\n",
    "print(listings_df.dropna(how='all',axis=1).shape)\n",
    "listings_df['scrape_id'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df['neighbourhood'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_df['neighbourhood'].value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this initial look at the dataset, some questions to consider include:\n",
    "\n",
    "    1) which variables are best predictors for the overall review score `review_score_rating`?\n",
    "    2) do review scores vary significantly between different neighborhoods?\n",
    "    3) which neighborhoods provide the best value (ratings/price ratio)?\n",
    "    4) if we control for the review_score_location, does the neighborhood rating change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hood_ratings = listings_df.groupby('neighbourhood').mean()['review_scores_rating'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_ratings.sort_values(by='review_scores_rating').head().style.format({'review_scores_rating': '{:.2f}'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hood_ratings.sort_values(by='review_scores_rating').tail().style.format({'review_scores_rating': '{:.2f}'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a map, so we can more easily see if there are any trends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = Basemap(projection='gnom', lat_0=57.3, lon_0=-6.2,\n",
    "            width=90000, height=120000, resolution=res, ax=ax[i])\n",
    "m.fillcontinents(color=\"#FFDDCC\", lake_color='#DDEEFF')\n",
    "m.drawmapboundary(fill_color=\"#DDEEFF\")\n",
    "m.drawcoastlines()\n",
    "ax[i].set_title(\"resolution='{0}'\".format(res));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_fit_linear_mod(df, response_col, cat_cols, dummy_na, test_size=.3, rand_state=42):\n",
    "    '''\n",
    "    INPUT:\n",
    "    df - a dataframe holding all the variables of interest\n",
    "    response_col - a string holding the name of the column \n",
    "    cat_cols - list of strings that are associated with names of the categorical columns\n",
    "    dummy_na - Bool holding whether you want to dummy NA vals of categorical columns or not\n",
    "    test_size - a float between [0,1] about what proportion of data should be in the test dataset\n",
    "    rand_state - an int that is provided as the random state for splitting the data into training and test \n",
    "    \n",
    "    OUTPUT:\n",
    "    test_score - float - r2 score on the test data\n",
    "    train_score - float - r2 score on the test data\n",
    "    lm_model - model object from sklearn\n",
    "    X_train, X_test, y_train, y_test - output from sklearn train test split used for optimal model\n",
    "    \n",
    "    Your function should:\n",
    "    1. Drop the rows with missing response values\n",
    "    2. Drop columns with NaN for all the values\n",
    "    3. Use create_dummy_df to dummy categorical columns\n",
    "    4. Fill the mean of the column for any missing values \n",
    "    5. Split your data into an X matrix and a response vector y\n",
    "    6. Create training and test sets of data\n",
    "    7. Instantiate a LinearRegression model with normalized data\n",
    "    8. Fit your model to the training data\n",
    "    9. Predict the response for the training data and the test data\n",
    "    10. Obtain an rsquared value for both the training and test data\n",
    "    '''\n",
    "    #Drop the rows with missing response values\n",
    "    df  = df.dropna(subset=[response_col], axis=0)\n",
    "\n",
    "    #Drop columns with all NaN values\n",
    "    df = df.dropna(how='all', axis=1)\n",
    "\n",
    "    #Dummy categorical variables\n",
    "    df = create_dummy_df(df, cat_cols, dummy_na)\n",
    "\n",
    "    # Mean function\n",
    "    fill_mean = lambda col: col.fillna(col.mean())\n",
    "    # Fill the mean\n",
    "    df = df.apply(fill_mean, axis=0)\n",
    "\n",
    "    #Split into explanatory and response variables\n",
    "    X = df.drop(response_col, axis=1)\n",
    "    y = df[response_col]\n",
    "\n",
    "    #Split into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=rand_state)\n",
    "\n",
    "    lm_model = LinearRegression(normalize=True) # Instantiate\n",
    "    lm_model.fit(X_train, y_train) #Fit\n",
    "\n",
    "    #Predict using your model\n",
    "    y_test_preds = lm_model.predict(X_test)\n",
    "    y_train_preds = lm_model.predict(X_train)\n",
    "\n",
    "    #Score using your model\n",
    "    test_score = r2_score(y_test, y_test_preds)\n",
    "    train_score = r2_score(y_train, y_train_preds)\n",
    "\n",
    "    return test_score, train_score, lm_model, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Test your function with the above dataset\n",
    "test_score, train_score, lm_model, X_train, X_test, y_train, y_test = clean_fit_linear_mod(df_new, 'Salary', cat_cols_lst, dummy_na=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
